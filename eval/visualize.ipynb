{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030cbf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "data_dir = \"/media/george/Data/mustc/en-de\"\n",
    "code_dir = \"/home/george/Projects/simulst\"\n",
    "fair_dir = \"/home/george/utility/fairseq\"\n",
    "sys.path.insert(0, code_dir)\n",
    "sys.path.insert(0, fair_dir)\n",
    "model = \"cif_de_sum_ctc0_3_lat0_0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2815ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import examples.simultaneous_translation\n",
    "from fairseq import (\n",
    "    checkpoint_utils,\n",
    "    options,\n",
    "    quantization_utils,\n",
    "    tasks,\n",
    "    utils,\n",
    ")\n",
    "from torchinfo import summary\n",
    "import logging\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from fairseq_cli.generate import get_symbols_to_strip_from_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fb1387",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = f\"{code_dir}/exp/checkpoints/{model}/checkpoint_best.pt\"\n",
    "use_cuda = True\n",
    "\n",
    "overrides = {\n",
    "    \"user_dir\": f\"{code_dir}/codebase\",\n",
    "    \"inference_config_yaml\": f\"{code_dir}/exp/infer_st.yaml\",\n",
    "    \"data\": data_dir,\n",
    "    \"gen_subset\": \"dev_st\",\n",
    "    \"batch_size\": 1,\n",
    "    \"beam\": 1,\n",
    "    \"do_mtl\": True,\n",
    "}\n",
    "\n",
    "states = checkpoint_utils.load_checkpoint_to_cpu(\n",
    "    path=checkpoint, arg_overrides=overrides, load_on_all_ranks=False)\n",
    "cfg = states[\"cfg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a074b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    level=os.environ.get(\"LOGLEVEL\", \"INFO\").upper(),\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "logger = logging.getLogger(\"fairseq_cli.train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dcf942",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.import_user_module(cfg.common)\n",
    "\n",
    "# Setup task, e.g., translation, language modeling, etc.\n",
    "task = tasks.setup_task(cfg.task)\n",
    "# Build model and criterion\n",
    "model = task.build_model(cfg.model)\n",
    "criterion = task.build_criterion(cfg.criterion)\n",
    "logger.info(summary(model))\n",
    "logger.info(\"task: {}\".format(task.__class__.__name__))\n",
    "logger.info(\"model: {}\".format(model.__class__.__name__))\n",
    "logger.info(\"criterion: {}\".format(criterion.__class__.__name__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be2d600",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"loading model(s) from {}\".format(cfg.common_eval.path))\n",
    "model = task.build_model(cfg.model)\n",
    "model.load_state_dict(\n",
    "    states[\"model\"], strict=True, model_cfg=cfg.model\n",
    ")\n",
    "\n",
    "# Optimize ensemble for generation\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "model.prepare_for_inference_(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9167bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "task.load_dataset(cfg.dataset.gen_subset, task_cfg=cfg.task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd070f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset \n",
    "itr = task.get_batch_iterator(\n",
    "    dataset=task.dataset(cfg.dataset.gen_subset),\n",
    "    max_tokens=cfg.dataset.max_tokens,\n",
    "    max_sentences=cfg.dataset.batch_size,\n",
    "    max_positions=utils.resolve_max_positions(\n",
    "        task.max_positions(), model.max_positions() #*[m.max_positions() for m in models]\n",
    "    ),\n",
    "    ignore_invalid_inputs=cfg.dataset.skip_invalid_size_inputs_valid_test,\n",
    "    required_batch_size_multiple=cfg.dataset.required_batch_size_multiple,\n",
    "    seed=cfg.common.seed,\n",
    "    num_shards=cfg.distributed_training.distributed_world_size,\n",
    "    shard_id=cfg.distributed_training.distributed_rank,\n",
    "    num_workers=cfg.dataset.num_workers,\n",
    "    data_buffer_size=cfg.dataset.data_buffer_size,\n",
    ").next_epoch_itr(shuffle=False)\n",
    "\n",
    "generator = task.build_generator(\n",
    "    [model], cfg.generation,\n",
    ")\n",
    "\n",
    "# Handle tokenization and BPE\n",
    "src_dict = task.source_dictionary\n",
    "tgt_dict = task.target_dictionary\n",
    "tokenizer = task.build_tokenizer(cfg.tokenizer)\n",
    "bpe = task.build_bpe(cfg.bpe)\n",
    "\n",
    "def encode_fn(x):\n",
    "    if tokenizer is not None:\n",
    "        x = tokenizer.encode(x)\n",
    "    if bpe is not None:\n",
    "        x = bpe.encode(x)\n",
    "    return x\n",
    "\n",
    "def decode_fn(x):\n",
    "    if bpe is not None:\n",
    "        x = bpe.decode(x)\n",
    "    if tokenizer is not None:\n",
    "        x = tokenizer.decode(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab5d940",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 480\n",
    "for sample in itr:\n",
    "    if use_cuda:\n",
    "        sample = utils.move_to_cuda(sample) \n",
    "    if \"net_input\" not in sample:\n",
    "        continue\n",
    "    if sample['id'].item() == idx:\n",
    "        break\n",
    "assert sample['id'].item() == idx\n",
    "sample_w_asr = sample\n",
    "sample = {\n",
    "    \"id\": sample_w_asr[\"id\"],\n",
    "    \"net_input\": {\n",
    "        \"src_tokens\": sample_w_asr[\"net_input\"][\"src_tokens\"],\n",
    "        \"src_lengths\": sample_w_asr[\"net_input\"][\"src_lengths\"],\n",
    "    }\n",
    "}\n",
    "translations = task.inference_step(\n",
    "    generator, [model], sample\n",
    ")\n",
    "utils.post_process_prediction(\n",
    "    hypo_tokens=translations[0][0][\"tokens\"].int().cpu(),\n",
    "    src_str=None,\n",
    "    alignment=None,\n",
    "    align_dict=None,\n",
    "    tgt_dict=tgt_dict,\n",
    "    remove_bpe=None, #\"sentencepiece\",\n",
    "    extra_symbols_to_ignore=get_symbols_to_strip_from_output(generator),\n",
    ")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d1aded",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.post_process_prediction(\n",
    "    hypo_tokens=translations[0][0][\"tokens\"].int().cpu(),\n",
    "    src_str=None,\n",
    "    alignment=None,\n",
    "    align_dict=None,\n",
    "    tgt_dict=tgt_dict,\n",
    "    remove_bpe=\"sentencepiece\",\n",
    "    extra_symbols_to_ignore=get_symbols_to_strip_from_output(generator),\n",
    ")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baa901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokens = sample[\"net_input\"][\"src_tokens\"]\n",
    "src_lengths = sample[\"net_input\"][\"src_lengths\"]\n",
    "target = translations[0][0][\"tokens\"].unsqueeze(0).type_as(src_lengths)\n",
    "prev_output_tokens = target.roll(1, 1)\n",
    "# logits, extra = model(src_tokens, src_lengths, prev_output_tokens=prev_output_tokens)\n",
    "extra = model.encoder(src_tokens, src_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cc82b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "with open(\"/home/george/Projects/simulst/eval/data/dev.wav_list\", \"r\") as f:\n",
    "    for l in f:\n",
    "        paths.append(l.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4198bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import IPython\n",
    "wav, sr = torchaudio.load(paths[idx])\n",
    "IPython.display.Audio(wav[:,:], rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7913e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = extra[\"alpha\"][0].detach()\n",
    "cif_steps = alpha.cumsum(-1).floor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adece86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfe4ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ccce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_checkpoint = f\"{code_dir}/exp/checkpoints/ctc_s2s_asr/checkpoint_best.pt\"\n",
    "asr_states = checkpoint_utils.load_checkpoint_to_cpu(\n",
    "    path=asr_checkpoint, arg_overrides=overrides, load_on_all_ranks=False)\n",
    "asr_cfg = asr_states[\"cfg\"]\n",
    "# Setup task, e.g., translation, language modeling, etc.\n",
    "asr_task = tasks.setup_task(asr_cfg.task)\n",
    "# Build model and criterion\n",
    "asr_model = asr_task.build_model(asr_cfg.model)\n",
    "asr_model.load_state_dict(\n",
    "    asr_states[\"model\"], strict=True, model_cfg=asr_cfg.model\n",
    ")\n",
    "\n",
    "# Optimize ensemble for generation\n",
    "if use_cuda:\n",
    "    asr_model.cuda()\n",
    "asr_model.prepare_for_inference_(asr_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaad148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from codebase.criterion.best_alignment import best_alignment\n",
    "encoder_out = asr_model.encoder(\n",
    "    sample['net_input']['src_tokens'],\n",
    "    sample['net_input']['src_lengths'],\n",
    ")\n",
    "ctc_logits = encoder_out[\"ctc_logits\"][0]\n",
    "encoder_mask = encoder_out[\"encoder_padding_mask\"][0]\n",
    "asr_target = sample_w_asr['net_input']['src_txt_tokens']\n",
    "asr_lenths = sample_w_asr['net_input']['src_txt_lengths']\n",
    "\n",
    "ctc_logits[..., 0] = ctc_logits[..., 0] - 10\n",
    "states = best_alignment(\n",
    "    ctc_logits.log_softmax(-1).transpose(0, 1),\n",
    "    asr_target,\n",
    "    (~encoder_mask).sum(-1),\n",
    "    asr_lenths,\n",
    "    blank=0,\n",
    "    as_labels=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fa4d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.style.use('seaborn')\n",
    "def print_segments(wav, states, labels, steps, preds, align):\n",
    "    padding = 5000\n",
    "    yscale = 0.6\n",
    "    texttop = 0.75\n",
    "    textbot = -0.75\n",
    "    font = 18\n",
    "    def ratio(x):\n",
    "        return (x + 1) * 4 * 10 * sr // 1000 - 1\n",
    "\n",
    "    # transcriptions\n",
    "    wav = wav * yscale\n",
    "    states = states.squeeze(0)\n",
    "    labels = labels.squeeze(0)\n",
    "    T = states.size(-1)\n",
    "\n",
    "    blanks = states % 2 == 0    \n",
    "    tgt_idx = states.div(2, rounding_mode='floor')\n",
    "    tgt_idx = tgt_idx.masked_fill(blanks, -1)\n",
    "    next_id = tgt_idx.roll(-1, dims=0)\n",
    "    prev_id = tgt_idx.roll(1, dims=0)\n",
    "\n",
    "    l_bound = torch.arange(T)[(tgt_idx != prev_id) & (~blanks)].tolist()\n",
    "    r_bound = torch.arange(T)[(tgt_idx != next_id) & (~blanks)].tolist()\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 8*yscale), dpi=100)\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(wav.squeeze().cpu().numpy() * texttop)\n",
    "    offset = texttop*yscale\n",
    "    ax.hlines(offset, colors=\"black\", xmin=0, xmax=ratio(T)+padding)\n",
    "    for l, r, idx in zip(l_bound, r_bound, labels):\n",
    "        if idx == src_dict.eos():\n",
    "            break\n",
    "        x0 = ratio(l)\n",
    "        x1 = ratio(r)\n",
    "        w = src_dict.string([idx], \"sentencepiece\")\n",
    "        ax.axvspan(x0, x1, alpha=0.1, color=\"red\", ymin=(textbot+1)/2) #, ymax=1 - (1-texttop) / 2)\n",
    "        ax.annotate(w, (x0, offset+0.05), ha=\"left\", fontsize=font)\n",
    "        \n",
    "    # translation\n",
    "    steps = steps.squeeze(0)\n",
    "    preds = preds.squeeze(0)\n",
    "    next_steps = steps.roll(-1, dims=0)\n",
    "    r_bound_trans = torch.arange(T)[(steps != next_steps)].tolist()\n",
    "    offset = textbot*yscale\n",
    "    ax.hlines(offset, colors=\"black\", xmin=0, xmax=ratio(T)+padding)\n",
    "    l_bound_trans = [0] + r_bound_trans\n",
    "    for l, r, idx in zip(l_bound_trans, r_bound_trans, preds):\n",
    "        if idx == tgt_dict.eos():\n",
    "            break\n",
    "        x0 = ratio(l)\n",
    "        x1 = ratio(r)\n",
    "        w = src_dict.string([idx], \"sentencepiece\")\n",
    "        ax.annotate(w, ((x0 + x1) / 2, offset-0.1), ha=\"center\", fontsize=font)\n",
    "        ax.vlines(x1, colors=\"black\", ymin=-1, ymax=offset)\n",
    "        \n",
    "    next_j = [a[1] for a in align[1:]] + [align[-1][1]]\n",
    "    for (i, j), n_j in zip(align, next_j):\n",
    "        src = ratio((l_bound[i] + r_bound[i]) / 2)\n",
    "        tgt = ratio((l_bound_trans[j] + r_bound_trans[j]) / 2)\n",
    "        ax.plot((src, tgt), (texttop*yscale, textbot*yscale), ':m' if j <= n_j else '-m')\n",
    "        \n",
    "\n",
    "    xticks = ax.get_xticks()\n",
    "    plt.xticks(xticks, (xticks * 1000 / sr).astype(int), fontsize=font*0.9)\n",
    "    ax.set_xlabel(\"Time (ms)\", fontsize=font*0.9)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylim(-yscale, yscale)\n",
    "    ax.set_xlim(0, wav.size(-1)+padding)\n",
    "    fig.savefig(\"policy.pdf\", bbox_inches='tight', pad_inches=0.05)\n",
    "\n",
    "\n",
    "align = [\n",
    "    (0, 0), (1, 1), (2, 3), (3, 4), (4, 7), (5, 5), (6, 6), (7, 8),\n",
    "    (8, 9), (9, 9), (10, 10), (11, 11), (12, 12), (13, 13), (14, 14)\n",
    "]\n",
    "print_segments(wav, states, asr_target, cif_steps, target, align)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7915ed25",
   "metadata": {},
   "source": [
    "0   1     2         3    4    5        6     7  8   9 10  11   12       13     14\n",
    "▁We ▁know ▁that ▁money ▁is ▁very ▁important , ▁go al s ▁are ▁very ▁important .\n",
    "0      1     2    3     4     5      6       7  8   9   10  11     12     13    14\n",
    "▁Wir ▁wissen , ▁dass ▁Geld ▁sehr ▁wichtig ▁ist , ▁Ziel e ▁sind ▁sehr ▁wichtig ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99169ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.post_process_prediction(\n",
    "    hypo_tokens=asr_target[0].cpu(),\n",
    "    src_str=None,\n",
    "    alignment=None,\n",
    "    align_dict=None,\n",
    "    tgt_dict=tgt_dict,\n",
    "    remove_bpe=None, #\"sentencepiece\",\n",
    "    extra_symbols_to_ignore=get_symbols_to_strip_from_output(generator),\n",
    ")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db08b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(wav[:,15359:19199], rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc0d046",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.post_process_prediction(\n",
    "    hypo_tokens=translations[0][0][\"tokens\"].int().cpu(),\n",
    "    src_str=None,\n",
    "    alignment=None,\n",
    "    align_dict=None,\n",
    "    tgt_dict=tgt_dict,\n",
    "    remove_bpe=None, #\"sentencepiece\",\n",
    "    extra_symbols_to_ignore=get_symbols_to_strip_from_output(generator),\n",
    ")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8324bdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ctc_logits = extra[\"ctc_logits\"][0]\n",
    "encoder_mask = extra[\"encoder_padding_mask\"][0]\n",
    "states = best_alignment(\n",
    "    ctc_logits.log_softmax(-1).transpose(0, 1),\n",
    "    target,\n",
    "    (~encoder_mask).sum(-1),\n",
    "    target.ne(1).sum(-1),\n",
    "    blank=0,\n",
    "    as_labels=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577dca9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609ee8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "states // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4147bb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 8), dpi=100)\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(states.squeeze(0).cpu().numpy() // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcac07ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_boundary(wav, states.div(2, rounding_mode='floor'), (states % 2 == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b6e838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04a13f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "data = {}\n",
    "with open(\"mustc_de-results/cif_de_sum_ctc0_3_lat0_0/instances.log\", \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        d = json.loads(line)\n",
    "        data[i] = {\n",
    "            \"prediction\": d[\"prediction\"],\n",
    "            \"reference\": d[\"reference\"],\n",
    "            \"bleu\": d[\"metric\"][\"sentence_bleu\"],\n",
    "            \"AL\": d[\"metric\"][\"latency\"][\"AL\"],\n",
    "            \"reference_length\": d[\"reference_length\"],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a887d160",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(data, orient='index').sort_values(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047aafb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fair]",
   "language": "python",
   "name": "conda-env-fair-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
